{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148d331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the person (Enter 'q' to quit): pratik\n",
      "Image 1 captured\n",
      "Image 2 captured\n",
      "Image 3 captured\n",
      "Image 4 captured\n",
      "Image 5 captured\n",
      "Image 6 captured\n",
      "Image 7 captured\n",
      "Image 8 captured\n",
      "Image 9 captured\n",
      "Image 10 captured\n",
      "Image 11 captured\n",
      "Image 12 captured\n",
      "Image 13 captured\n",
      "Image 14 captured\n",
      "Image 15 captured\n",
      "Image 16 captured\n",
      "Image 17 captured\n",
      "Image 18 captured\n",
      "Image 19 captured\n",
      "Image 20 captured\n",
      "Image 21 captured\n",
      "Image 22 captured\n",
      "Image 23 captured\n",
      "Image 24 captured\n",
      "Image 25 captured\n",
      "Image 26 captured\n",
      "Image 27 captured\n",
      "Image 28 captured\n",
      "Image 29 captured\n",
      "Image 30 captured\n",
      "Image 31 captured\n",
      "Image 32 captured\n",
      "Image 33 captured\n",
      "Image 34 captured\n",
      "Image 35 captured\n",
      "Image 36 captured\n",
      "Image 37 captured\n",
      "Image 38 captured\n",
      "Image 39 captured\n",
      "Image 40 captured\n",
      "Image 41 captured\n",
      "Image 42 captured\n",
      "Image 43 captured\n",
      "Image 44 captured\n",
      "Image 45 captured\n",
      "Image 46 captured\n",
      "Image 47 captured\n",
      "Image 48 captured\n",
      "Image 49 captured\n",
      "Image 50 captured\n",
      "Image 51 captured\n",
      "Image 52 captured\n",
      "Image 53 captured\n",
      "Image 54 captured\n",
      "Image 55 captured\n",
      "Image 56 captured\n",
      "Image 57 captured\n",
      "Image 58 captured\n",
      "Image 59 captured\n",
      "Image 60 captured\n",
      "Image 61 captured\n",
      "Image 62 captured\n",
      "Image 63 captured\n",
      "Image 64 captured\n",
      "Image 65 captured\n",
      "Image 66 captured\n",
      "Image 67 captured\n",
      "Image 68 captured\n",
      "Image 69 captured\n",
      "Image 70 captured\n",
      "Image 71 captured\n",
      "Image 72 captured\n",
      "Image 73 captured\n",
      "Image 74 captured\n",
      "Image 75 captured\n",
      "Image 76 captured\n",
      "Image 77 captured\n",
      "Image 78 captured\n",
      "Image 79 captured\n",
      "Image 80 captured\n",
      "Image 81 captured\n",
      "Image 82 captured\n",
      "Image 83 captured\n",
      "Image 84 captured\n",
      "Image 85 captured\n",
      "Image 86 captured\n",
      "Image 87 captured\n",
      "Image 88 captured\n",
      "Image 89 captured\n",
      "Image 90 captured\n",
      "Image 91 captured\n",
      "Image 92 captured\n",
      "Image 93 captured\n",
      "Image 94 captured\n",
      "Image 95 captured\n",
      "Image 96 captured\n",
      "Image 97 captured\n",
      "Image 98 captured\n",
      "Image 99 captured\n",
      "Image 100 captured\n",
      "Image 101 captured\n",
      "Image 102 captured\n",
      "Image 103 captured\n",
      "Image 104 captured\n",
      "Image 105 captured\n",
      "Image 106 captured\n",
      "Image 107 captured\n",
      "Image 108 captured\n",
      "Image 109 captured\n",
      "Image 110 captured\n",
      "Image 111 captured\n",
      "Image 112 captured\n",
      "Image 113 captured\n",
      "Image 114 captured\n",
      "Image 115 captured\n",
      "Image 116 captured\n",
      "Image 117 captured\n",
      "Image 118 captured\n",
      "Image 119 captured\n",
      "Image 120 captured\n",
      "Image 121 captured\n",
      "Image 122 captured\n",
      "Image 123 captured\n",
      "Image 124 captured\n",
      "Image 125 captured\n",
      "Image 126 captured\n",
      "Image 127 captured\n",
      "Image 128 captured\n",
      "Image 129 captured\n",
      "Image 130 captured\n",
      "Image 131 captured\n",
      "Image 132 captured\n",
      "Image 133 captured\n",
      "Image 134 captured\n",
      "Image 135 captured\n",
      "Image 136 captured\n",
      "Image 137 captured\n",
      "Image 138 captured\n",
      "Image 139 captured\n",
      "Image 140 captured\n",
      "Image 141 captured\n",
      "Image 142 captured\n",
      "Image 143 captured\n",
      "Image 144 captured\n",
      "Image 145 captured\n",
      "Image 146 captured\n",
      "Image 147 captured\n",
      "Image 148 captured\n",
      "Image 149 captured\n",
      "Image 150 captured\n",
      "Image 151 captured\n",
      "Image 152 captured\n",
      "Image 153 captured\n",
      "Image 154 captured\n",
      "Image 155 captured\n",
      "Image 156 captured\n",
      "Image 157 captured\n",
      "Image 158 captured\n",
      "Image 159 captured\n",
      "Image 160 captured\n",
      "Image 161 captured\n",
      "Image 162 captured\n",
      "Image 163 captured\n",
      "Image 164 captured\n",
      "Image 165 captured\n",
      "Image 166 captured\n",
      "Image 167 captured\n",
      "Image 168 captured\n",
      "Image 169 captured\n",
      "Image 170 captured\n",
      "Image 171 captured\n",
      "Image 172 captured\n",
      "Image 173 captured\n",
      "Image 174 captured\n",
      "Image 175 captured\n",
      "Image 176 captured\n",
      "Image 177 captured\n",
      "Image 178 captured\n",
      "Image 179 captured\n",
      "Image 180 captured\n",
      "Image 181 captured\n",
      "Image 182 captured\n",
      "Image 183 captured\n",
      "Image 184 captured\n",
      "Image 185 captured\n",
      "Image 186 captured\n",
      "Image 187 captured\n",
      "Image 188 captured\n",
      "Image 189 captured\n",
      "Image 190 captured\n",
      "Image 191 captured\n",
      "Image 192 captured\n",
      "Image 193 captured\n",
      "Image 194 captured\n",
      "Image 195 captured\n",
      "Image 196 captured\n",
      "Image 197 captured\n",
      "Image 198 captured\n",
      "Image 199 captured\n",
      "Image 200 captured\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# we have created Function to create directory if it doesn't exist\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# it is Function to capture images from webcam, detect faces, crop them, and save\n",
    "def capture_images(label_name):\n",
    "    # Check if the label name is 'q'\n",
    "    if label_name.lower() == 'q':\n",
    "        print(\"Exiting the program.\")\n",
    "        return\n",
    "    \n",
    "    # Create directory for the label (person's name)\n",
    "    create_directory('dataset/' + label_name)\n",
    "    \n",
    "    # Load pre-trained face detection model\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Start capturing video from webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Check if the webcam is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Failed to open webcam.\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "        \n",
    "        # Convert frame to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        # Draw rectangles around detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Capturing Images - Press \"c\" to capture (Press \"q\" to quit)', frame)\n",
    "\n",
    "        # Press 'c' to capture photo\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('c'):\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Crop the face\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Save the cropped face in the corresponding label folder\n",
    "                img_path = f'dataset/{label_name}/img_{count}.jpg'\n",
    "                cv2.imwrite(img_path, face_roi)\n",
    "                count += 1\n",
    "                print(f\"Image {count} captured\")\n",
    "\n",
    "                # Draw rectangle around the captured face\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Limit to 200 images\n",
    "        if count >= 200:\n",
    "            break\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Input label name (person's name)\n",
    "    label_name = input(\"Enter the name of the person (Enter 'q' to quit): \")\n",
    "\n",
    "    # Capture images, detect faces, crop them, and save\n",
    "    capture_images(label_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff717ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Function to collect images and create dataset\n",
    "def create_dataset(dataset_directory):\n",
    "    # Open or create a CSV file to store image paths and labels\n",
    "    with open('dataset.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['image_path', 'label'])\n",
    "\n",
    "        # Traverse the dataset directory to collect images and assign labels\n",
    "        for label_name in os.listdir(dataset_directory):\n",
    "            label_directory = os.path.join(dataset_directory, label_name)\n",
    "            if os.path.isdir(label_directory):\n",
    "                for image_name in os.listdir(label_directory):\n",
    "                    image_path = os.path.join(label_directory, image_name)\n",
    "                    # Write image path and label to the CSV file\n",
    "                    csv_writer.writerow([image_path, label_name])\n",
    "\n",
    "def start():\n",
    "    # Directory to save the dataset\n",
    "    dataset_directory = 'dataset'\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(dataset_directory):\n",
    "        os.makedirs(dataset_directory)\n",
    "\n",
    "    # Collect images and create dataset\n",
    "    create_dataset(dataset_directory)\n",
    "\n",
    "    # Your other code here...\n",
    "\n",
    "\n",
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8e8fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Model saved to face_recognition_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(dataset_file):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines[1:]:\n",
    "            image_path, label = line.strip().split(',')\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # Resize the image to match the input size of the model\n",
    "            resized_image = cv2.resize(image, (160, 160))\n",
    "            X.append(resized_image.flatten())\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def start():\n",
    "    # Directory to save the dataset\n",
    "    dataset_directory = 'dataset'\n",
    "\n",
    "    # Create the dataset\n",
    "\n",
    "\n",
    "    # Path to the dataset CSV file\n",
    "    dataset_file = 'dataset.csv'\n",
    "\n",
    "    # Load the dataset\n",
    "    X, y = load_dataset(dataset_file)\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize SVM classifier\n",
    "    clf = SVC(kernel='linear')\n",
    "\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict labels for test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_file = 'face_recognition_model.joblib'\n",
    "    dump(clf, model_file)\n",
    "    print('Model saved to', model_file)\n",
    "\n",
    "\n",
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68b9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from joblib import load\n",
    "\n",
    "def start():\n",
    "    # Load the trained model from disk\n",
    "    model_file = 'face_recognition_model.joblib'\n",
    "    clf = load(model_file)\n",
    "\n",
    "    # Initialize the webcam or video capture device\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # Load the pre-trained face cascade classifier\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Iterate over each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face region from the frame\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            # Resize the face region to match the input size of the model\n",
    "            face_roi_resized = cv2.resize(face_roi, (160, 160))\n",
    "\n",
    "            # Flatten the face region\n",
    "            face_features = face_roi_resized.flatten()\n",
    "\n",
    "            # Predict the label (name) of the person\n",
    "            predicted_label = clf.predict([face_features])[0]\n",
    "\n",
    "            # Draw a rectangle around the detected face and display the predicted label\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture device and close all OpenCV windows\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae69285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
